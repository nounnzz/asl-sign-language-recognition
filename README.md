# ü§ü Real-Time ASL Sign Language Recognition

A real-time American Sign Language (ASL) alphabet recognizer using **MediaPipe Hands** + **TensorFlow/Keras**. Classifies all 24 static ASL letters (A through Y, excluding J and Z which require motion).

---

## üìÅ Project Structure

```
asl_sign_language/
‚îú‚îÄ‚îÄ asl_recognition.py    ‚Üê Main real-time inference app
‚îú‚îÄ‚îÄ train_model.py        ‚Üê Model training script
‚îú‚îÄ‚îÄ collect_data.py       ‚Üê Webcam data collection tool
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ landmarks.csv     ‚Üê Your collected training data (created by collect_data.py)
‚îî‚îÄ‚îÄ model/
    ‚îú‚îÄ‚îÄ asl_model.keras   ‚Üê Trained model (created by train_model.py)
    ‚îî‚îÄ‚îÄ training_curves.png
```

---

## ‚öôÔ∏è Setup

### 1. Create a virtual environment (recommended)
```bash
python -m venv venv
venv\Scripts\activate           # Windows
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

> **Python 3.9‚Äì3.11 recommended.** TensorFlow 2.x supports these versions best.

---

## Collect your own data

```bash
# Step 1: Collect ~100 samples per letter via webcam
python collect_data.py --samples 100

# Step 2: Train the model
python train_model.py --data data/landmarks.csv --epochs 40

# Step 3: Run real-time recognition
python asl_recognition.py
```

---

## üéÆ Controls (asl_recognition.py)

| Key       | Action                              |
|-----------|-------------------------------------|
| `SPACE`   | Add current stable letter to word   |
| `BKSP`    | Delete last letter from word        |
| `C`       | Clear word buffer                   |
| `Q / ESC` | Quit                                |

---

## üìä Data Format

`data/landmarks.csv` - each row is one sample:

```
label, f0, f1, f2, ..., f62
A, 0.012, -0.043, 0.001, ...   ‚Üê 63 normalized landmark values
B, 0.023, -0.011, 0.002, ...
```

- **Label**: Capital letter (A-Y, excluding J)
- **f0‚Äìf62**: 63 normalized landmark coordinates (21 landmarks √ó x, y, z), wrist-relative and unit-scaled

To convert the Sign Language MNIST (pixel images) to landmark CSV, use MediaPipe to extract landmarks from each image and save in this format.

---

## üß† How It Works

```
Webcam frame
     ‚îÇ
     ‚ñº
MediaPipe Hands
     ‚îÇ  detects 21 hand keypoints (x, y, z)
     ‚ñº
Landmark Normalization
     ‚îÇ  subtract wrist, divide by max abs value ‚Üí 63 features
     ‚ñº
Keras MLP Classifier
     ‚îÇ  Input(63) ‚Üí Dense(256) ‚Üí BN ‚Üí Dropout
     ‚îÇ            ‚Üí Dense(128) ‚Üí BN ‚Üí Dropout
     ‚îÇ            ‚Üí Dense(64)  ‚Üí Dropout
     ‚îÇ            ‚Üí Dense(24, softmax)
     ‚ñº
Smoothing (majority vote, window=5)
     ‚îÇ
     ‚ñº
Predicted ASL Letter + Confidence
```

**Why landmarks instead of raw pixels?**
- Lighting invariant
- Position / scale invariant
- ~10√ó faster than CNN on images
- Very lightweight model (~200KB)

---

## üõ† Tuning Options

```bash
# Adjust confidence threshold (default 0.80)
python asl_recognition.py --confidence 0.85

# Use different camera (e.g., external webcam)
python asl_recognition.py --camera 1

# Adjust smoothing window
python asl_recognition.py --smooth 7

# Full options
python asl_recognition.py --help

```

---

## Notable Dependencies

| Package         | Role                                |
|-----------------|-------------------------------------|
| `mediapipe`     | Real-time hand landmark detection   |
| `tensorflow`    | Keras model training & inference    |
| `opencv-python` | Webcam capture & UI rendering       |
| `scikit-learn`  | Train/val split, label encoding     |
| `matplotlib`    | Training curve plots                |

---

## Possible Extensions

- Add **J and Z** using landmark motion/velocity features
- Add **word prediction** using a language model
- Export to **TensorFlow Lite** for mobile deployment
- Add **sentence builder** with grammar suggestions
- Build a **web app** using TensorFlow.js

---

## ‚ö†Ô∏è Disclaimers

- **J and Z** are excluded because they require hand motion (temporal information), not just a static pose.
- The `model/asl_model.keras` file is generated by `train_model.py` - it is not included in the repo.
- For best results, collect data in similar lighting conditions to where you'll use the app.
